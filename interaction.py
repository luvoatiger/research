import os
import numpy as np
import torch
import torch.nn as nn
from torchdde import integrate, AdaptiveStepSizeController, Dopri5
from tqdm import tqdm
import json
import time
import matplotlib.pyplot as plt
from scipy.integrate import solve_ivp

# ===========================
# 1. NDDE ëª¨ë¸ ì •ì˜ (ìˆ˜í•™ì  ì •ì˜ì— ë”°ë¥¸ êµ¬í˜„)
# ===========================
class NDDE(nn.Module):
    """
    Neural Delay Differential Equation (NDDE) ëª¨ë¸
    ìˆ˜í•™ì  ì •ì˜: dy/dt = f_theta(t, y(t), y(t-tau_1), ..., y(t-tau_n))

    Args:
        delays: ì§€ì—° ì‹œê°„ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ [tau_1, tau_2, ..., tau_n]
        in_size: y(t)ì˜ ì°¨ì›
        out_size: dy/dtì˜ ì°¨ì› (ë³´í†µ in_sizeì™€ ê°™ìŒ)
        width_size: ì€ë‹‰ì¸µì˜ ë„ˆë¹„
        depth: ì€ë‹‰ì¸µì˜ ê¹Šì´
    """
    def __init__(self, delays, in_size, out_size, width_size=128, depth=3):
        super().__init__()
        self.delays = delays
        self.in_size = in_size
        self.out_size = out_size

        # ì…ë ¥ ì°¨ì›: í˜„ì¬ ìƒíƒœ + ëª¨ë“  ì§€ì—° ìƒíƒœë“¤
        # in_dim = in_size * (1 + len(delays))
        self.in_dim = in_size * (1 + len(delays))

        # MLP êµ¬ì„±: depthê°œì˜ ì€ë‹‰ì¸µ + ì¶œë ¥ì¸µ
        layers = []
        # ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ
        layers.append(nn.Linear(self.in_dim, width_size))
        layers.append(nn.LeakyReLU())

        # ì¤‘ê°„ ì€ë‹‰ì¸µë“¤
        for _ in range(depth - 1):
            layers.append(nn.Linear(width_size, width_size))
            layers.append(nn.LeakyReLU())

        # ì¶œë ¥ì¸µ
        layers.append(nn.Linear(width_size, out_size))

        self.mlp = nn.Sequential(*layers)

        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.apply(self._init_weights)

    def _init_weights(self, module):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight, gain=0.01)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)

    def forward(self, t, z, func_args, *, history):
        """
        NDDE forward pass

        Args:
            t: í˜„ì¬ ì‹œê°„
            z: í˜„ì¬ ìƒíƒœ y(t) [batch_size, in_size]
            func_args: ì¶”ê°€ ì¸ìˆ˜ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            history: ì§€ì—° ìƒíƒœë“¤ì˜ ë¦¬ìŠ¤íŠ¸ [y(t-tau_1), y(t-tau_2), ..., y(t-tau_n)]
                    ê°ê° [batch_size, in_size] í˜•íƒœ

        Returns:
            dy/dt: [batch_size, out_size]
        """
        # ì…ë ¥ ë°ì´í„° ê²€ì¦
        if torch.isnan(z).any() or torch.isinf(z).any():
            print(f"ê²½ê³ : NDDE í˜„ì¬ ìƒíƒœì— NaN/Infê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
            return torch.zeros(z.shape[0], self.out_size, device=z.device)

        for i, hist in enumerate(history):
            if torch.isnan(hist).any() or torch.isinf(hist).any():
                print(f"ê²½ê³ : NDDE ì§€ì—° ìƒíƒœ {i}ì— NaN/Infê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
                return torch.zeros(z.shape[0], self.out_size, device=z.device)

        # í˜„ì¬ ìƒíƒœì™€ ëª¨ë“  ì§€ì—° ìƒíƒœë“¤ì„ ì—°ê²°
        # torch.cat([z, *history], dim=-1) í˜•íƒœë¡œ êµ¬í˜„
        concatenated = torch.cat([z, *history], dim=-1)

        # MLPë¥¼ í†µí•œ f_theta ê³„ì‚°
        output = self.mlp(concatenated)

        # ì¶œë ¥ ë°ì´í„° ê²€ì¦
        if torch.isnan(output).any() or torch.isinf(output).any():
            print(f"ê²½ê³ : NDDE ì¶œë ¥ì— NaN/Infê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
            return torch.zeros(z.shape[0], self.out_size, device=z.device)

        return output


# ===========================
# 2. Lorenz 96 ì‹œìŠ¤í…œê³¼ NDDE ê²°í•©
# ===========================
class Lorenz96NDDE(nn.Module):
    """
    Lorenz 96 ì‹œìŠ¤í…œê³¼ NDDEë¥¼ ê²°í•©í•œ ëª¨ë¸
    dx/dt = Lorenz96_markov(x) + NDDE_memory(x, x(t-tau_1), ..., x(t-tau_n))
    """
    def __init__(self, delays, K, J, F, h, b, c, width_size=128, depth=3):
        super().__init__()
        self.K = K
        self.J = J
        self.delays = delays

        # Lorenz 96 ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° (metadataì—ì„œ ê°€ì ¸ì˜´)
        self.F = F      # ê°•ì œ í•­ íŒŒë¼ë¯¸í„°
        self.h = h      # ê²°í•© ê°•ë„
        self.b = b      # Y ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§
        self.c = c      # Y ë³€ìˆ˜ ì‹œê°„ ìŠ¤ì¼€ì¼

        # NDDE ë©”ëª¨ë¦¬ í•­
        self.ndde = NDDE(delays=delays, in_size=K, out_size=K,
                        width_size=width_size, depth=depth)

    def forward(self, t, x, func_args, *, history):
        """
        Lorenz 96 + NDDE forward pass

        Args:
            t: í˜„ì¬ ì‹œê°„
            x: í˜„ì¬ ìƒíƒœ [batch_size, K]
            func_args: ì¶”ê°€ ì¸ìˆ˜
            history: ì§€ì—° ìƒíƒœë“¤ì˜ ë¦¬ìŠ¤íŠ¸

        Returns:
            dx/dt: [batch_size, K]
        """
        # Lorenz 96 Markov í•­ ê³„ì‚°
        markov_term = self.lorenz96_markov(x)

        # NDDE ë©”ëª¨ë¦¬ í•­ ê³„ì‚°
        memory_term = self.ndde(t, x, func_args, history=history)

        # ì „ì²´ ë¯¸ë¶„ ê³„ì‚°
        dxdt = markov_term + memory_term

        return dxdt

    def lorenz96_markov(self, x):
        """
        Lorenz 96 ì‹œìŠ¤í…œì˜ Markov í•­ ê³„ì‚°
        metadataì˜ íŒŒë¼ë¯¸í„°ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ Lorenz 96 ì‹œìŠ¤í…œ êµ¬í˜„
        """
        K = x.shape[1]
        roll_p1 = torch.roll(x, shifts=-1, dims=1)
        roll_m2 = torch.roll(x, shifts=2, dims=1)
        roll_m1 = torch.roll(x, shifts=1, dims=1)

        # ì •í™•í•œ Lorenz 96 Markov í•­ ê³„ì‚°
        # dX[k] = (X[(k+1) % K] - X[(k-2) % K]) * X[(k-1) % K] - X[k] + F
        result = (roll_p1 - roll_m2) * roll_m1 - x + self.F

        return result


# ===========================
# 3. ë°ì´í„° ì „ì²˜ë¦¬
# ===========================
def load_l96_data(data_dir: str, N: int = 300):
    """
    Lorenz 96 ë°ì´í„° ë¡œë”©

    Args:
        data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        N (int): ë¡œë”©í•  ë°°ì¹˜ ìˆ˜

    Returns:
        list: ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    data_list = []
    print(f"[+] ë°ì´í„° ë¡œë”© ì‹œì‘: {data_dir}")

    for i in range(1, N + 1):
        x_data_path = os.path.join(data_dir, f"X_batch_{i}.npy")
        t_data_path = os.path.join(data_dir, f"t_batch_{i}.npy")
        if not os.path.exists(x_data_path):
            print(f"ê²½ê³ : íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {x_data_path}")
            continue

        try:
            X = np.load(x_data_path)[0]  # shape: [T, K]
            t = np.load(t_data_path)[0]  # shape: [T]

            # ë°ì´í„° ê²€ì¦
            if np.isnan(X).any() or np.isinf(X).any():
                print(f"ê²½ê³ : ë°°ì¹˜ {i}ì— NaN/Infê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
                print(f"ë°ì´í„° ë²”ìœ„: [{X.min():.4f}, {X.max():.4f}]")
                continue

            data_list.append((X, t) )

        except Exception as e:
            print(f"ê²½ê³ : ë°°ì¹˜ {i} ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

    print(f"[+] ì„±ê³µì ìœ¼ë¡œ ë¡œë”©ëœ ë°°ì¹˜ ìˆ˜: {len(data_list)}/{N}")
    if len(data_list) == 0:
        raise ValueError("ë¡œë”©ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")

    return data_list


def preprocess_trajectories_for_ndde(data_list, sequence_length=200, num_sequences_per_traj=3, dt=0.01, seed=42, use_real_time=False):
    """
    ë¡œë”©ëœ trajectoryë“¤ì„ Neural DDE í•™ìŠµì— ë§ê²Œ ì „ì²˜ë¦¬
    ê° trajectoryì—ì„œ ëœë¤í•˜ê²Œ ìŠ¬ë¼ì´ì‹±í•˜ì—¬ sequenceë“¤ì„ ìƒì„±

    Args:
        data_list: load_l96_dataë¡œ ë¡œë”©ëœ trajectory ë¦¬ìŠ¤íŠ¸ (X, t) íŠœí”Œ
        sequence_length: ê° sequenceì˜ ê¸¸ì´
        num_sequences_per_traj: ê° trajectoryì—ì„œ ì¶”ì¶œí•  sequence ê°œìˆ˜
        dt: ì‹œê°„ ê°„ê²© (use_real_time=Falseì¼ ë•Œë§Œ ì‚¬ìš©)
        seed: ëœë¤ ì‹œë“œ
        use_real_time: ì‹¤ì œ ì‹œê°„ ë°ì´í„° ì‚¬ìš© ì—¬ë¶€

    Returns:
        tuple: (ts, ys) - ì‹œê°„ êµ¬ê°„ê³¼ ì „ì²˜ë¦¬ëœ sequenceë“¤
    """
    np.random.seed(seed)
    print(f"[+] Neural DDEìš© ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
    print(f"  - ì›ë³¸ trajectory ìˆ˜: {len(data_list)}")
    print(f"  - Sequence ê¸¸ì´: {sequence_length}")
    print(f"  - Trajectoryë‹¹ sequence ìˆ˜: {num_sequences_per_traj}")
    print(f"  - ì‹¤ì œ ì‹œê°„ ì‚¬ìš©: {use_real_time}")

    processed_sequences = []
    processed_times = []  # ì‹¤ì œ ì‹œê°„ ë°ì´í„° ì €ì¥
    total_sequences = 0

    for i, (X, t) in enumerate(data_list):
        # ë°ì´í„° ê²€ì¦
        if np.isnan(X).any() or np.isinf(X).any():
            print(f"ê²½ê³ : trajectory {i}ì— NaN/Infê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
            continue

        T = len(X)
        
        # sequence_lengthë³´ë‹¤ ì§§ì€ trajectoryëŠ” ê±´ë„ˆë›°ê¸°
        if T < sequence_length:
            print(f"ê²½ê³ : trajectory {i}ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ê¸¸ì´: {T})")
            continue

        # ê²¹ì¹˜ì§€ ì•ŠëŠ” ëœë¤ ì‹œì‘ì ë“¤ ìƒì„±
        max_start_idx = T - sequence_length
        if max_start_idx < num_sequences_per_traj:
            # ê°€ëŠ¥í•œ sequence ìˆ˜ê°€ ìš”ì²­ëœ ìˆ˜ë³´ë‹¤ ì ì€ ê²½ìš°
            num_sequences = max_start_idx + 1
            start_indices = list(range(num_sequences))
        else:
            # ëœë¤í•˜ê²Œ ì‹œì‘ì  ì„ íƒ (ê²¹ì¹˜ì§€ ì•Šë„ë¡)
            start_indices = np.random.choice(max_start_idx + 1, 
                                           size=min(num_sequences_per_traj, max_start_idx + 1), 
                                           replace=False)

        # ê° ì‹œì‘ì ì—ì„œ sequence ì¶”ì¶œ
        for start_idx in start_indices:
            end_idx = start_idx + sequence_length
            
            # sequence ì¶”ì¶œ
            sequence = X[start_idx:end_idx]
            
            # ë°ì´í„° ê²€ì¦
            if np.isnan(sequence).any() or np.isinf(sequence).any():
                print(f"ê²½ê³ : trajectory {i}ì˜ sequence {start_idx}-{end_idx}ì— NaN/Infê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
                continue
            
            processed_sequences.append(sequence)
            
            # ì‹¤ì œ ì‹œê°„ ë°ì´í„°ë„ ì¶”ì¶œ
            if use_real_time:
                time_sequence = t[start_idx:end_idx]
                processed_times.append(time_sequence)
            
            total_sequences += 1

    if len(processed_sequences) == 0:
        raise ValueError("ì „ì²˜ë¦¬ëœ ìœ íš¨í•œ sequenceê°€ ì—†ìŠµë‹ˆë‹¤!")

    # ì‹œê°„ êµ¬ê°„ ìƒì„±
    if use_real_time:
        # ì‹¤ì œ ì‹œê°„ ë°ì´í„° ì‚¬ìš© (ì²« ë²ˆì§¸ sequenceì˜ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ)
        ts = torch.tensor(processed_times[0], dtype=torch.float32)
        print(f"  - ì‹¤ì œ ì‹œê°„ ë²”ìœ„: [{ts[0]:.4f}, {ts[-1]:.4f}]")
    else:
        # ê· ë“±í•œ ì‹œê°„ ê°„ê²© ìƒì„±
        ts = torch.linspace(0, (sequence_length-1) * dt, sequence_length)
        print(f"  - ê· ë“± ì‹œê°„ ê°„ê²©: [{ts[0]:.4f}, {ts[-1]:.4f}]")

    # ë°°ì¹˜ë¡œ ìŠ¤íƒ
    ys = torch.tensor(np.stack(processed_sequences), dtype=torch.float32)  # [N, T, K]

    print(f"[+] ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ:")
    print(f"  - ìƒì„±ëœ sequence ìˆ˜: {ys.shape[0]}")
    print(f"  - ì‹œê°„ ìŠ¤í…: {ys.shape[1]}")
    print(f"  - ë³€ìˆ˜ ìˆ˜: {ys.shape[2]}")
    print(f"  - ë°ì´í„° ë²”ìœ„: [{ys.min():.4f}, {ys.max():.4f}]")
    print(f"  - ì´ sequence ìˆ˜: {total_sequences}")

    return ts, ys


def create_fixed_delays(dt=0.01, num_delays=5):
    """
    ê³ ì •ëœ ì§€ì—° ì‹œê°„ ìƒì„± (dtì— ë§ê²Œ ì„¤ì •)

    Args:
        dt: ì‹œê°„ ê°„ê²©
        num_delays: ì§€ì—° ì‹œê°„ ê°œìˆ˜

    Returns:
        torch.tensor: ê³ ì •ëœ ì§€ì—° ì‹œê°„ë“¤
    """
    # dtì— ë§ê²Œ 1, 2, 3, 4, 5 ìŠ¤í… ë’¤ì˜ ê³¼ê±° ê°’ë“¤ì„ ê°€ì ¸ì˜¤ë„ë¡ ì„¤ì •
    delays = torch.tensor([np.round(dt * i, 4) for i in range(1, num_delays + 1)], dtype=torch.float32)

    print(f"[+] ê³ ì • ì§€ì—° ì‹œê°„ ìƒì„±:")
    print(f"  - ì§€ì—° ì‹œê°„ ê°œìˆ˜: {num_delays}")
    print(f"  - ì‹œê°„ ê°„ê²© (dt): {dt}")
    print(f"  - ì§€ì—° ì‹œê°„ë“¤: {delays.tolist()}")

    return delays


class Lorenz96Dataset(torch.utils.data.Dataset):
    """
    Lorenz 96 ë°ì´í„°ì…‹
    """
    def __init__(self, ys):
        self.ys = ys

    def __getitem__(self, index):
        return self.ys[index]

    def __len__(self):
        return self.ys.shape[0]


# ===========================
# 5. NDDE í•™ìŠµ í•¨ìˆ˜ (torchdde ê¸°ë°˜)
# ===========================
def train_lorenz96_ndde(
    metadata,
    dataset,
    delays,
    batch_size=32,
    lr=0.001,
    max_epoch=50,
    width_size=128,
    depth=3,
    seed=42,
    plot=True,
    print_every=5,
    device="cpu"
):
    """
    Lorenz96NDDE ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ (torchdde ê¸°ë°˜)

    Args:
        metadata: Lorenz 96 ì‹œìŠ¤í…œ ë©”íƒ€ë°ì´í„°
        dataset: ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹
        delays: í•™ìŠµ ê°€ëŠ¥í•œ ì§€ì—° ì‹œê°„ íŒŒë¼ë¯¸í„°
        batch_size: ë°°ì¹˜ í¬ê¸°
        lr: í•™ìŠµë¥ 
        max_epoch: ìµœëŒ€ ì—í¬í¬ ìˆ˜
        width_size: ì€ë‹‰ì¸µ ë„ˆë¹„
        depth: ì€ë‹‰ì¸µ ê¹Šì´
        seed: ëœë¤ ì‹œë“œ
        plot: ì‹œê°í™” ì—¬ë¶€
        print_every: ë¡œê¹… ê°„ê²©
        device: ë””ë°”ì´ìŠ¤

    Returns:
        tuple: (ts, ys, model, losses, delays_evol)
    """
    torch.manual_seed(seed)

    # ë°ì´í„° ì¶”ì¶œ
    ts = dataset.ts
    ys = dataset.ys

    # ëª¨ë¸ ìƒì„±
    K = metadata['K']
    J = metadata['J']
    F = metadata['F']
    h = metadata['h']
    b = metadata['b']
    c = metadata['c']

    model = Lorenz96NDDE(
        delays=delays,
        K=K, J=J, F=F, h=h, b=b, c=c,
        width_size=width_size, depth=depth
    ).to(device)

    print(f"[+] ëª¨ë¸ êµ¬ì„± ì™„ë£Œ:")
    print(f"  - ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters())}")

    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # í•™ìŠµ ì„¤ì •
    losses, delays_evol = [], []
    loss_fn = torch.nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)

    print(f"[+] í•™ìŠµ ì‹œì‘: {max_epoch} ì—í¬í¬, ë°°ì¹˜ í¬ê¸° {batch_size}")

    # í•™ìŠµ ë£¨í”„
    for epoch in tqdm(range(max_epoch)):
        model.train()
        epoch_losses = []

        for step, data in enumerate(train_loader):
            start_time = time.time()
            optimizer.zero_grad()

            data = data.to(device)  # [batch_size, T, K]

            # History í•¨ìˆ˜: ê° ë°°ì¹˜ì˜ ì²« ë²ˆì§¸ ì‹œê°„ ìŠ¤í…ì„ ì´ˆê¸° ì¡°ê±´ìœ¼ë¡œ ì‚¬ìš©
            history_fn = lambda t: data[:, 0]  # [batch_size, K]

            try:
                # NDDE í†µí•©
                ys_pred = integrate(
                    model,
                    Dopri5(),
                    ts[0],
                    ts[-1],
                    ts,
                    history_fn,
                    func_args=None,
                    dt0=ts[1] - ts[0],
                    stepsize_controller=AdaptiveStepSizeController(1e-6, 1e-9),
                    discretize_then_optimize=True,
                    delays=delays,
                )

                # Loss ê³„ì‚°
                loss = loss_fn(ys_pred, data)
                epoch_losses.append(loss.item())

                # ì—­ì „íŒŒ
                loss.backward()

                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

                optimizer.step()

                # ì§€ì—° ì‹œê°„ì€ ê³ ì •ì´ë¯€ë¡œ ê¸°ë¡í•˜ì§€ ì•ŠìŒ
                # delays_evol.append(delays.clone().detach())  # ì£¼ì„ ì²˜ë¦¬

            except Exception as e:
                print(f"ê²½ê³ : ë°°ì¹˜ {step} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue

        # ì—í¬í¬ í‰ê·  ì†ì‹¤ ê³„ì‚°
        if epoch_losses:
            avg_loss = np.mean(epoch_losses)
            losses.append(avg_loss)
            scheduler.step(avg_loss)

            # ë¡œê¹…
            if (epoch % print_every) == 0 or epoch == max_epoch - 1:
                print(
                    f"Epoch: {epoch:3d}/{max_epoch}, "
                    f"Loss: {avg_loss:.6f}, "
                    f"LR: {optimizer.param_groups[0]['lr']:.2e}"
                )

    print(f"[+] í•™ìŠµ ì™„ë£Œ! ìµœì¢… ì†ì‹¤: {losses[-1]:.6f}")

    # ì‹œê°í™”
    if plot:
        plot_training_results(ts, ys, model, losses, [], device)  # delays_evolì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬

    return ts, ys, model, losses, []


def plot_training_results(ts, ys, model, losses, delays_evol, device):
    """
    í•™ìŠµ ê²°ê³¼ ì‹œê°í™”
    """
    model.eval()

    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
    with torch.no_grad():
        test_data = ys[0:1].to(device)  # ì²« ë²ˆì§¸ trajectory
        history_fn = lambda t: test_data[:, 0]

        ys_pred = integrate(
            model,
            Dopri5(),
            ts[0],
            ts[-1],
            ts,
            history_fn,
            func_args=None,
            dt0=ts[1] - ts[0],
            stepsize_controller=AdaptiveStepSizeController(1e-6, 1e-9),
            delays=model.delays,
        )

    # ë³€ìˆ˜ ìˆ˜ ê³„ì‚°
    num_variables = ys.shape[2]
    
    # ì‹œê°í™” 1: ëª¨ë“  ë³€ìˆ˜ë“¤ì˜ ê°œë³„ subplot
    fig1, axes = plt.subplots(2, 4, figsize=(20, 10))
    axes = axes.flatten()
    
    for i in range(num_variables):
        ax = axes[i]
        ax.plot(ts.cpu(), ys_pred[0, :, i].cpu(), '--', c='red', label='NDDE Prediction', linewidth=2)
        ax.plot(ts.cpu(), test_data[0, :, i].cpu(), '-', c='blue', label='Ground Truth', linewidth=2)
        ax.set_xlabel('Time t')
        ax.set_ylabel(f'X_{i+1}(t)')
        ax.set_title(f'Variable X_{i+1}: Prediction vs Ground Truth')
        ax.legend()
        ax.grid(True)
    
    plt.tight_layout()
    plt.savefig('lorenz96_ndde_all_variables.png', dpi=300, bbox_inches='tight')
    plt.show()

    # ì‹œê°í™” 2: í•™ìŠµ ê²°ê³¼ ìš”ì•½
    fig2, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. ì†ì‹¤ ê³¡ì„ 
    axes[0, 0].plot(losses)
    axes[0, 0].set_yscale('log')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_title('Training Loss')
    axes[0, 0].grid(True)

    # 2. ì˜ˆì¸¡ ì˜¤ì°¨
    error = (ys_pred[0] - test_data[0]).abs().mean(dim=1)
    axes[0, 1].plot(ts.cpu(), error.cpu())
    axes[0, 1].set_xlabel('Time t')
    axes[0, 1].set_ylabel('Mean Absolute Error')
    axes[0, 1].set_title('Prediction Error over Time')
    axes[0, 1].grid(True)

    # 3. RMSE by Variable
    mse = torch.mean((ys_pred[0] - test_data[0]) ** 2, dim=0)
    rmse = torch.sqrt(mse)
    axes[1, 0].bar(range(1, len(rmse) + 1), rmse.cpu())
    axes[1, 0].set_xlabel('Variable Index')
    axes[1, 0].set_ylabel('RMSE')
    axes[1, 0].set_title('RMSE by Variable')
    axes[1, 0].grid(True)

    # 4. ëª¨ë“  ë³€ìˆ˜ ë¹„êµ (í•˜ë‚˜ì˜ ê·¸ë˜í”„ì—)
    for i in range(num_variables):
        axes[1, 1].plot(ts.cpu(), ys_pred[0, :, i].cpu(), '--', linewidth=1, alpha=0.7, label=f'Pred X_{i+1}')
        axes[1, 1].plot(ts.cpu(), test_data[0, :, i].cpu(), '-', linewidth=1, alpha=0.7, label=f'True X_{i+1}')
    axes[1, 1].set_xlabel('Time t')
    axes[1, 1].set_ylabel('X(t)')
    axes[1, 1].set_title('All Variables Comparison')
    axes[1, 1].legend()
    axes[1, 1].grid(True)

    plt.tight_layout()
    plt.savefig('lorenz96_ndde_training_summary.png', dpi=300, bbox_inches='tight')
    plt.show()


    # ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥
    print(f"\n[+] ìµœì¢… ì„±ëŠ¥ ì§€í‘œ:")
    print(f"  - ì „ì²´ RMSE: {torch.sqrt(torch.mean((ys_pred[0] - test_data[0]) ** 2)):.6f}")

    for i in range(len(rmse)):
        print(f"  - ë³€ìˆ˜ {i+1} RMSE: {rmse[i]:.6f}")


# ===========================
# 6. ì „ì²´ ì‹¤í–‰
# ===========================
if __name__ == "__main__":
    # -------------------------------
    # âš™ï¸ ì„¤ì •
    # -------------------------------
    data_dir = os.path.join(os.getcwd(), "simulated_data")  # ë°ì´í„° ê²½ë¡œ
    hidden_dim = 128
    epochs = 100
    device = "cpu"
    dropout_rate = 0.1
    sequence_length = 200  # ê° sequenceì˜ ê¸¸ì´
    num_sequences_per_traj = 3  # ê° trajectoryì—ì„œ ì¶”ì¶œí•  sequence ê°œìˆ˜
    use_real_time = False  # ì‹¤ì œ ì‹œê°„ ë°ì´í„° ì‚¬ìš© ì—¬ë¶€ (True: ì‹¤ì œ ì‹œê°„, False: ê· ë“± ê°„ê²©)

    # -------------------------------
    # ğŸ“‚ ë°ì´í„° ë¡œë”©
    # -------------------------------
    data_list = load_l96_data(data_dir)

    # -------------------------------
    # ğŸ“‚ ë©”íƒ€ë°ì´í„° ë¡œë”©
    # -------------------------------
    try:
        # ë©”íƒ€ë°ì´í„° ë¡œë”©
        with open(os.path.join(data_dir, "metadata.json"), "r") as f:
            metadata = json.load(f)

        # ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        K = metadata['K']
        J = metadata['J']
        F = metadata['F']
        h = metadata['h']
        b = metadata['b']
        c = metadata['c']
        dt = metadata['dt']

        print(f"[+] ë©”íƒ€ë°ì´í„° ë¡œë”© ì™„ë£Œ:")
        print(f"  - K (X ë³€ìˆ˜ ìˆ˜): {K}")
        print(f"  - J (Y ë³€ìˆ˜ ìˆ˜): {J}")
        print(f"  - F (ê°•ì œ í•­): {F}")
        print(f"  - h (ê²°í•© ê°•ë„): {h}")
        print(f"  - b (Y ìŠ¤ì¼€ì¼ë§): {b}")
        print(f"  - c (Y ì‹œê°„ ìŠ¤ì¼€ì¼): {c}")
        print(f"  - dt (ì‹œê°„ ê°„ê²©): {dt}")

    except Exception as e:
        print(f"ì˜¤ë¥˜: ë©”íƒ€ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
        exit(1)

    # -------------------------------
    # ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬
    # -------------------------------
    try:
        print(f"[+] ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")

        # Neural DDEìš© ë°ì´í„° ì „ì²˜ë¦¬
        ts, ys = preprocess_trajectories_for_ndde(
            data_list,
            sequence_length=sequence_length,
            num_sequences_per_traj=num_sequences_per_traj,
            dt=dt,
            seed=42,
            use_real_time=use_real_time
        )

        # ë°ì´í„°ì…‹ ìƒì„±
        dataset = Lorenz96Dataset(ys)
        dataset.ts = ts  # ì‹œê°„ êµ¬ê°„ì„ ë°ì´í„°ì…‹ì— ì¶”ê°€

        print(f"[+] ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ:")
        print(f"  - ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}")
        print(f"  - ë°ì´í„° í˜•íƒœ: {ys.shape}")

    except Exception as e:
        print(f"ì˜¤ë¥˜: ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

    # -------------------------------
    # â° ì§€ì—° ì‹œê°„ íŒŒë¼ë¯¸í„° ìƒì„±
    # -------------------------------
    try:
        # ê³ ì •ëœ ì§€ì—° ì‹œê°„ ìƒì„±
        delays = create_fixed_delays(
            dt=dt,
            num_delays=5
        )

    except Exception as e:
        print(f"ì˜¤ë¥˜: ì§€ì—° ì‹œê°„ ìƒì„± ì‹¤íŒ¨: {e}")
        exit(1)

    # -------------------------------
    # ğŸ§  NDDE ëª¨ë¸ í•™ìŠµ
    # -------------------------------
    try:
        print(f"[+] ì„¤ì •:")
        print(f"  - ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
        print(f"  - ì€ë‹‰ì¸µ ì°¨ì›: {hidden_dim}")
        print(f"  - í•™ìŠµ ì—í¬í¬: {epochs}")
        print(f"  - ë””ë°”ì´ìŠ¤: {device}")
        print(f"  - Dropout ë¹„ìœ¨: {dropout_rate}")
        print(f"  - Sequence ê¸¸ì´: {sequence_length}")
        print(f"  - Trajectoryë‹¹ sequence ìˆ˜: {num_sequences_per_traj}")
        print(f"  - ì‹¤ì œ ì‹œê°„ ì‚¬ìš©: {use_real_time}")

        # Lorenz96NDDE í•™ìŠµ
        ts, ys, model, losses, delays_evol = train_lorenz96_ndde(
            metadata=metadata,
            dataset=dataset,
            delays=delays,
            batch_size=512,    # ì‘ì€ ë°°ì¹˜ í¬ê¸°
            lr=0.001,
            max_epoch=epochs,
            width_size=hidden_dim,
            depth=3,
            seed=42,
            plot=True,
            print_every=2,
            device=device
        )

        print(f"[+] NDDE í•™ìŠµ ì™„ë£Œ!")
        print(f"  - ìµœì¢… ì†ì‹¤: {losses[-1]:.6f}")
        print(f"  - ê³ ì • ì§€ì—° ì‹œê°„: {delays.tolist()}")

        # ëª¨ë¸ ì €ì¥
        torch.save({
            'model_state_dict': model.state_dict(),
            'metadata': metadata,
            'losses': losses,
            'delays_evol': delays_evol,
            'final_delays': delays.tolist()
        }, 'lorenz96_ndde_model.pth')
        print(f"[+] ëª¨ë¸ì´ lorenz96_ndde_model.pthì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"ì˜¤ë¥˜: NDDE í•™ìŠµ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        exit(1)